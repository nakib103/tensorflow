{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eager_execution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdrVmlwmXss3xJXRf/RePL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nakib103/tensorflow/blob/master/eager_execution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZe738uHfguX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import cProfile\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r01C1ZzghMrW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edc6009b-96ba-4248-9a54-ab960b026781"
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KI_DxwMhO6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7bed460-3400-4318-ec93-986c8dbd7c65"
      },
      "source": [
        "x = [[2.]]\n",
        "print(type(x))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RWZJlt5hanA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63258870-b777-4f7d-9d06-0ab2da971e27"
      },
      "source": [
        "# tf.Tensor objects reference concrete values instead of symbolic handles to nodes in a computational graph\n",
        "m = tf.matmul(x, x)\n",
        "# Since there isn't a computational graph to build and run later in a session, it's easy to inspect results using print() or a debugger. \n",
        "print(\"Matric multiplication result {}\".format(m))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matric multiplication result [[4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BPJndxnhl1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f6023570-2e43-4390-ae64-3711736d286a"
      },
      "source": [
        "a = tf.constant([[1., 3.],\n",
        "                 [2., 4.]])\n",
        "\n",
        "b = tf.constant([[3., 1.],\n",
        "                 [5., 2.]])\n",
        "\n",
        "print(a * b)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 3.  3.]\n",
            " [10.  8.]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVrx8sGKhqW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "960c7b5c-ff86-4325-d3dc-7d883de0aba0"
      },
      "source": [
        "# Eager execution works nicely with NumPy. NumPy operations accept tf.Tensor arguments\n",
        "import numpy as np\n",
        "c = np.multiply(a, b)\n",
        "print(c)\n",
        "\n",
        "print(a.numpy())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3.  3.]\n",
            " [10.  8.]]\n",
            "[[1. 3.]\n",
            " [2. 4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO49JY7uj45U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# functionality of the host language is available while your model is executing.\n",
        "\n",
        "def iffneff(num):\n",
        "  counter = tf.constant(0)\n",
        "\n",
        "  max = tf.convert_to_tensor(num)\n",
        "  for idx in range(1, max.numpy()+1):\n",
        "    if(idx % 3 == 0):\n",
        "      print(\"iffff\")\n",
        "    else:\n",
        "      print(\"effff\")\n",
        "\n",
        "  counter += 1\n",
        "\n",
        "iffneff(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqrsUc_lk7Uy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79506857-8771-4395-86f9-1b0c6831f2ec"
      },
      "source": [
        "# During eager execution, use tf.GradientTape to trace operations for computing gradients later\n",
        "w = tf.Variable([3.])\n",
        "with tf.GradientTape() as tape:\n",
        "  mul = w * w * w\n",
        "\n",
        "grad = tape.gradient(mul, w)\n",
        "print(grad)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([27.], shape=(1,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmBbc56DybYe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "05d84809-ee83-445f-c5cf-f29d635d8b86"
      },
      "source": [
        "# Even without training, call the model and inspect the output in eager execution\n",
        "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\n",
        "   tf.cast(mnist_labels,tf.int64)))\n",
        "dataset = dataset.shuffle(1000).batch(32)\n",
        "\n",
        "mnist_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Conv2D(16,[3,3], activation='relu',\n",
        "                         input_shape=(None, None, 1)),\n",
        "  tf.keras.layers.Conv2D(16,[3,3], activation='relu'),\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "for images,labels in dataset.take(1):\n",
        "  print(\"Logits: \", mnist_model(images[0:1]).numpy())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Logits:  [[ 0.03187805  0.04470838 -0.02943167 -0.03389054 -0.03579982  0.04431757\n",
            "   0.03672335 -0.03304952 -0.01135808 -0.03420841]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wnZEmlPzh65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cfd0778f-0d55-47d6-aa44-58b5d2bc8d95"
      },
      "source": [
        "# training loop implemented with eager\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = mnist_model(images, training=True)\n",
        "    \n",
        "    # Add asserts to check the shape of the output.\n",
        "    tf.debugging.assert_equal(logits.shape, (32, 10))\n",
        "    \n",
        "    loss_value = loss_object(labels, logits)\n",
        "\n",
        "  loss_history.append(loss_value.numpy().mean())\n",
        "  grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
        "\n",
        "def train(epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for (batch, (images, labels)) in enumerate(dataset):\n",
        "      train_step(images, labels)\n",
        "    print ('Epoch {} finished'.format(epoch))\n",
        "\n",
        "train(epochs = 3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 finished\n",
            "Epoch 1 finished\n",
            "Epoch 2 finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcxUC7mZ1Cm0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "441cd6c7-0ad9-47fd-c690-dfd5704e047f"
      },
      "source": [
        "# tf.Variable objects store mutable tf.Tensor-like values accessed during training to make automatic differentiation easier\n",
        "class Linear(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Linear, self).__init__()            # calls super class init function\n",
        "    self.W = tf.Variable(5., name=\"weight\")\n",
        "    self.B = tf.Variable(10., name=\"bias\")\n",
        "\n",
        "  def call(self, inputs):                     # forward pass is defined in call method\n",
        "    return inputs * self.W + self.B\n",
        "\n",
        "def loss(model, inputs, targets):\n",
        "  error = model(inputs) - targets\n",
        "  return tf.reduce_mean(tf.square(error))\n",
        "\n",
        "def grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets)\n",
        "  return tape.gradient(loss_value, [model.W, model.B])\n",
        "\n",
        "NUM_EXAMPLES = 2000\n",
        "training_inputs = tf.random.normal([NUM_EXAMPLES])\n",
        "noise = tf.random.normal([NUM_EXAMPLES])\n",
        "training_outputs = training_inputs * 3 + 2 + noise\n",
        "\n",
        "model = Linear()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
        "\n",
        "steps = 300\n",
        "for i in range(steps):\n",
        "  grads = grad(model, training_inputs, training_outputs)\n",
        "  optimizer.apply_gradients(zip(grads, [model.W, model.B]))\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial loss: 69.235\n",
            "Loss at step 000: 66.534\n",
            "Loss at step 020: 30.224\n",
            "Loss at step 040: 14.034\n",
            "Loss at step 060: 6.814\n",
            "Loss at step 080: 3.595\n",
            "Loss at step 100: 2.159\n",
            "Loss at step 120: 1.519\n",
            "Loss at step 140: 1.233\n",
            "Loss at step 160: 1.106\n",
            "Loss at step 180: 1.049\n",
            "Loss at step 200: 1.023\n",
            "Loss at step 220: 1.012\n",
            "Loss at step 240: 1.007\n",
            "Loss at step 260: 1.005\n",
            "Loss at step 280: 1.004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMjdSWwr5kuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# object-based saving\n",
        "# object-oriented metrics\n",
        "# summaries and tensorboard\n",
        "\n",
        "# dynamic model\n",
        "# custom gradients\n",
        "# performance\n",
        "# benchmarks\n",
        "# work with functions"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}